{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13699476,"sourceType":"datasetVersion","datasetId":8714293}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T23:12:28.964812Z","iopub.execute_input":"2025-11-16T23:12:28.965759Z","iopub.status.idle":"2025-11-16T23:12:29.508246Z","shell.execute_reply.started":"2025-11-16T23:12:28.965724Z","shell.execute_reply":"2025-11-16T23:12:29.507430Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"42066"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# üéÉ TWO-SENTENCE HORROR STORY GENERATOR - MISTRAL 7B VERSION\n# Fine-tuning Mistral 7B on Reddit r/TwoSentenceHorror with instruction format\n# ============================================================================\n# KAGGLE SETUP:\n# 1. Settings ‚Üí Accelerator ‚Üí GPU T4 x2\n# 2. Settings ‚Üí Persistence ‚Üí Files only\n# 3. Add datasets: historical-two-sentence-horror-split\n# 4. Run this entire notebook\n# ============================================================================\n\n# CRITICAL: Set this BEFORE any other imports!\nimport os\n# Use BOTH GPUs for more memory!\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"  # Changed from just \"0\"\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# ============================================================================\n# üìã CONFIGURATION\n# ============================================================================\n\n# Which dataset part are you training on?\nDATASET_PART = 1  # Start fresh with Part 1\n\n# Resume from checkpoint?\nCONTINUE_FROM_PREVIOUS = False  # Set True for Parts 2, 3, 4\nPREVIOUS_MODEL_PATH = None  # Update when continuing training\n\n# Output directory\nOUTPUT_DIR = f\"/kaggle/working/llama-horror-part{DATASET_PART}\"\n\n# Memory optimization\nUSE_FLASH_ATTENTION = False  # Disabled - not installed on Kaggle\nMAX_SEQ_LENGTH = 384  # Increased since we have 2 GPUs now\n\n# ============================================================================\n# 1. INSTALL LIBRARIES\n# ============================================================================\nprint(\"üì¶ Installing required libraries...\")\n!pip install -q --upgrade transformers datasets accelerate bitsandbytes peft trl\n\n# ============================================================================\n# 2. IMPORTS\n# ============================================================================\nimport torch\nimport time\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom trl import SFTTrainer\n\nprint(f\"‚úÖ PyTorch version: {torch.__version__}\")\nprint(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    gpu_count = torch.cuda.device_count()\n    print(f\"‚úÖ GPU Count: {gpu_count}\")\n    for i in range(gpu_count):\n        print(f\"‚úÖ GPU {i}: {torch.cuda.get_device_name(i)}\")\n        print(f\"   Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB\")\n\n# ============================================================================\n# 3. LOAD DATASET\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(f\"üìä LOADING DATASET PART {DATASET_PART}\")\nprint(\"=\"*70)\n\ndata_file_path = f\"/kaggle/input/historical-two-sentence-horror-split/dataset_part_{DATASET_PART}.txt\"\n\nprint(f\"Loading from: {data_file_path}\")\nraw_dataset = load_dataset(\"text\", data_files={\"train\": data_file_path}, split=\"train\")\n\nprint(f\"‚úÖ Loaded {len(raw_dataset)} stories\")\nprint(f\"\\nüìñ Raw sample:\")\nprint(raw_dataset[0]['text'][:200] + \"...\")\n\n# ============================================================================\n# 4. FORMAT DATASET WITH INSTRUCTION TEMPLATE\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"üîß FORMATTING DATASET WITH INSTRUCTION TEMPLATE\")\nprint(\"=\"*70)\n\ndef format_instruction(example):\n    \"\"\"\n    Convert raw horror story into Mistral instruction format\n    This teaches the model to follow instructions, not just complete text\n    \"\"\"\n    story = example['text'].strip()\n    \n    # Mistral instruction format\n    formatted = f\"\"\"<s>[INST] Write a creative and chilling two-sentence horror story. [/INST] {story}</s>\"\"\"\n    \n    return {\"text\": formatted}\n\n# Apply formatting to entire dataset\nformatted_dataset = raw_dataset.map(format_instruction)\n\nprint(f\"‚úÖ Formatted {len(formatted_dataset)} stories\")\nprint(f\"\\nüìñ Formatted sample:\")\nprint(formatted_dataset[0]['text'][:300] + \"...\")\n\n# ============================================================================\n# 5. LOAD MODEL\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nif CONTINUE_FROM_PREVIOUS:\n    print(f\"üîÑ LOADING PREVIOUS MODEL\")\nelse:\n    print(f\"ü§ñ LOADING MISTRAL 7B INSTRUCT\")\nprint(\"=\"*70)\n\nmodel_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n\n# 4-bit quantization config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16 for Llama\n    bnb_4bit_use_double_quant=True,  # Extra memory savings\n)\n\nif CONTINUE_FROM_PREVIOUS and PREVIOUS_MODEL_PATH:\n    print(f\"\\nüìÇ Loading previous model from: {PREVIOUS_MODEL_PATH}\")\n    \n    # Load base model\n    print(\"   Step 1/3: Loading base model...\")\n    base_model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        quantization_config=bnb_config,\n        device_map=\"cuda:0\",\n        trust_remote_code=True,\n        max_memory={0: \"14GB\"},\n    )\n    \n    # Load LoRA adapter\n    print(\"   Step 2/3: Loading LoRA adapter...\")\n    model = PeftModel.from_pretrained(\n        base_model,\n        PREVIOUS_MODEL_PATH,\n        is_trainable=True,\n    )\n    \n    print(\"   Step 3/3: Enabling training mode...\")\n    model.train()\n    \n    # Enable gradients\n    for param in model.parameters():\n        if param.requires_grad:\n            param.data = param.data.to(torch.bfloat16)\n    \n    print(f\"   ‚úÖ Previous model loaded\")\n    \nelse:\n    print(f\"\\nüÜï Loading fresh {model_id}...\")\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id,\n        quantization_config=bnb_config,\n        device_map=\"auto\",  # Automatically split across both GPUs!\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n    )\n    \n    print(\"‚úÖ Base model loaded\")\n    print(f\"üìä Model device map: {model.hf_device_map}\")  # Show which layers on which GPU\n\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Don't run prepare_model_for_kbit_training - it causes OOM\n# The model is already prepared from quantization_config\nprint(\"‚úÖ Model prepared for training (skipping k-bit prep to save memory)\")\n\n# Load tokenizer\nprint(\"\\nüìù Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nprint(f\"‚úÖ Model size: ~{sum(p.numel() for p in model.parameters()) / 1e9:.2f}B parameters\")\n\n# ============================================================================\n# 6. LORA CONFIGURATION\n# ============================================================================\nif not CONTINUE_FROM_PREVIOUS:\n    print(\"\\n\" + \"=\"*70)\n    print(\"‚öôÔ∏è  CONFIGURING LORA\")\n    print(\"=\"*70)\n    \n    peft_config = LoraConfig(\n        r=8,                          # Back to 8 since we have 2 GPUs now\n        lora_alpha=16,                # 2x rank\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=[              # More modules now that we have memory\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n        ],\n    )\n    \n    # Apply LoRA\n    model = get_peft_model(model, peft_config)\n    model.print_trainable_parameters()\n    \n    print(\"‚úÖ LoRA config applied\")\n    print(f\"   - Rank: {peft_config.r}\")\n    print(f\"   - Alpha: {peft_config.lora_alpha}\")\n    print(f\"   - Target modules: {len(peft_config.target_modules)} modules\")\nelse:\n    peft_config = None\n    print(\"\\n‚úÖ Using existing LoRA configuration from previous model\")\n\n# ============================================================================\n# 7. TRAINING ARGUMENTS\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"üéØ TRAINING CONFIGURATION\")\nprint(\"=\"*70)\n\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    num_train_epochs=2,\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=4,    # Back to 4 with 2 GPUs\n    learning_rate=1e-4,\n    fp16=False,\n    bf16=True,\n    optim=\"paged_adamw_8bit\",\n    logging_steps=25,\n    save_steps=400,\n    save_total_limit=3,\n    report_to=\"none\",\n    warmup_steps=100,\n    weight_decay=0.01,\n    max_grad_norm=0.3,\n    dataloader_pin_memory=False,\n    group_by_length=True,\n    gradient_checkpointing=True,      # Still keep this for safety\n)\n\nprint(\"‚úÖ Training arguments set\")\nprint(f\"   - Dataset: Part {DATASET_PART} ({len(formatted_dataset)} stories)\")\nprint(f\"   - Epochs: {training_args.num_train_epochs}\")\nprint(f\"   - Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\nprint(f\"   - Learning rate: {training_args.learning_rate}\")\nprint(f\"   - Save every: {training_args.save_steps} steps\")\nprint(f\"   - Max sequence length: {MAX_SEQ_LENGTH}\")\nprint(f\"   - Gradient checkpointing: ENABLED (saves memory)\")\nprint(f\"   - Output: {OUTPUT_DIR}\")\n\n# ============================================================================\n# 8. INITIALIZE TRAINER\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"üöÄ INITIALIZING TRAINER\")\nprint(\"=\"*70)\n\n# Formatting function for SFTTrainer\ndef formatting_prompts_func(example):\n    \"\"\"Return the formatted text for training\"\"\"\n    return example[\"text\"]\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=formatted_dataset,\n    args=training_args,\n    peft_config=peft_config,\n    formatting_func=formatting_prompts_func,\n)\n\nprint(\"‚úÖ Trainer initialized\")\n\n# ============================================================================\n# 9. TRAIN!\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nif CONTINUE_FROM_PREVIOUS:\n    print(f\"üî• CONTINUING TRAINING FROM PREVIOUS MODEL\")\nelse:\n    print(f\"üî• STARTING FRESH TRAINING\")\nprint(f\"üìö Training on {len(formatted_dataset)} stories from Part {DATASET_PART}\")\nprint(\"=\"*70)\nprint(\"‚è∞ Estimated time: ~45-60 minutes...\")\nprint(\"üí° Checkpoints will be saved every 300 steps\")\nprint()\n\nstart_time = time.time()\n\ntry:\n    trainer.train()\n    training_time = time.time() - start_time\n    \n    print(\"\\n\" + \"=\"*70)\n    print(f\"‚úÖ TRAINING COMPLETE! Time: {training_time/60:.1f} minutes\")\n    print(\"=\"*70)\n    \nexcept Exception as e:\n    training_time = time.time() - start_time\n    print(\"\\n\" + \"=\"*70)\n    print(f\"‚ùå TRAINING FAILED AFTER {training_time/60:.1f} minutes\")\n    print(f\"Error: {e}\")\n    print(\"=\"*70)\n\n# ============================================================================\n# 10. SAVE FINAL MODEL\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"üíæ SAVING FINAL MODEL\")\nprint(\"=\"*70)\n\nfinal_output_dir = f\"{OUTPUT_DIR}/final-model\"\nos.makedirs(final_output_dir, exist_ok=True)\n\ntry:\n    trainer.model.save_pretrained(final_output_dir)\n    print(\"‚úÖ Model saved\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è Save failed: {e}\")\n\ntokenizer.save_pretrained(final_output_dir)\nprint(\"‚úÖ Tokenizer saved\")\n\ntime.sleep(2)\n\n# ============================================================================\n# 11. VERIFY SAVES\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"üìÅ CHECKING SAVED FILES\")\nprint(\"=\"*70)\n\nif os.path.exists(OUTPUT_DIR):\n    checkpoints = [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")]\n    if checkpoints:\n        print(f\"\\n‚úÖ Found {len(checkpoints)} checkpoint(s):\")\n        for cp in sorted(checkpoints):\n            print(f\"   üìÇ {cp}\")\n\nif os.path.exists(final_output_dir):\n    print(f\"\\n‚úÖ Final model directory: {final_output_dir}\")\n    for item in sorted(os.listdir(final_output_dir))[:10]:\n        if os.path.isfile(os.path.join(final_output_dir, item)):\n            size = os.path.getsize(os.path.join(final_output_dir, item))\n            print(f\"   üìÑ {item} ({size/1024/1024:.1f} MB)\")\n\n# ============================================================================\n# 12. TEST THE MODEL\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"üß™ TESTING MODEL WITH SAMPLE PROMPTS\")\nprint(\"=\"*70)\n\n# Test prompts in proper instruction format\ntest_prompts = [\n    \"Write a creative and chilling two-sentence horror story about a mother and daughter.\",\n    \"Write a creative and chilling two-sentence horror story about technology.\",\n    \"Write a creative and chilling two-sentence horror story about being alone at night.\",\n]\n\ntuned_model = trainer.model\ntuned_model.eval()\n\nfor i, user_prompt in enumerate(test_prompts, 1):\n    print(f\"\\n{'='*70}\")\n    print(f\"Test {i}/{len(test_prompts)}\")\n    print(f\"{'='*70}\")\n    print(f\"üé≠ PROMPT: {user_prompt}\\n\")\n    \n    # Format in Mistral instruction template\n    full_prompt = f\"\"\"<s>[INST] {user_prompt} [/INST]\"\"\"\n    \n    try:\n        inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(tuned_model.device)\n        \n        with torch.no_grad():\n            outputs = tuned_model.generate(\n                **inputs,\n                max_new_tokens=100,\n                temperature=0.8,          # Lower for more coherent output\n                top_p=0.92,\n                do_sample=True,\n                repetition_penalty=1.3,\n                pad_token_id=tokenizer.eos_token_id,\n                eos_token_id=tokenizer.eos_token_id,\n            )\n        \n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n        \n        # Extract just the assistant's response\n        if \"assistant\" in generated_text:\n            response = generated_text.split(\"assistant\")[-1].strip()\n            print(f\"üëª GENERATED STORY:\\n{response}\\n\")\n        else:\n            print(f\"üëª GENERATED:\\n{generated_text}\\n\")\n            \n    except Exception as e:\n        print(f\"‚ùå Generation failed: {e}\\n\")\n\n# ============================================================================\n# 13. FINAL SUMMARY\n# ============================================================================\nprint(\"\\n\" + \"=\"*70)\nprint(\"üéâ SUMMARY\")\nprint(\"=\"*70)\nprint(f\"‚úÖ Model: Mistral 7B Instruct\")\nprint(f\"‚úÖ Dataset: Part {DATASET_PART} ({len(formatted_dataset)} stories)\")\nprint(f\"‚úÖ Training mode: {'Continued from previous' if CONTINUE_FROM_PREVIOUS else 'Fresh training'}\")\nprint(f\"‚úÖ Output directory: {OUTPUT_DIR}\")\nprint(f\"‚úÖ Final model: {final_output_dir}\")\n\nprint(\"\\nüìù NEXT STEPS:\")\nprint(\"   1. Click 'Save Version' to save this notebook\")\nprint(\"   2. Create a dataset from the output\")\nprint(\"   3. For next part, update:\")\nprint(f\"      - DATASET_PART = {DATASET_PART + 1}\")\nprint(f\"      - CONTINUE_FROM_PREVIOUS = True\")\nprint(f\"      - PREVIOUS_MODEL_PATH = '/kaggle/input/your-saved-model/final-model'\")\n\nprint(\"\\nüí° MISTRAL 7B ADVANTAGES:\")\nprint(\"   ‚úì Fits comfortably on single T4 GPU\")\nprint(\"   ‚úì Excellent instruction following\")\nprint(\"   ‚úì More coherent than Phi-3\")\nprint(\"   ‚úì Fast training (~45-60 min per part)\")\nprint(\"   ‚úì Great balance of quality and efficiency\")\n\nprint(\"\\n\" + \"=\"*70)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T23:30:38.864693Z","iopub.execute_input":"2025-11-16T23:30:38.865567Z"}},"outputs":[{"name":"stdout","text":"üì¶ Installing required libraries...\n‚úÖ PyTorch version: 2.6.0+cu124\n‚úÖ CUDA available: True\n‚úÖ GPU Count: 2\n‚úÖ GPU 0: Tesla T4\n   Memory: 14.7 GB\n‚úÖ GPU 1: Tesla T4\n   Memory: 14.7 GB\n\n======================================================================\nüìä LOADING DATASET PART 1\n======================================================================\nLoading from: /kaggle/input/historical-two-sentence-horror-split/dataset_part_1.txt\n‚úÖ Loaded 20000 stories\n\nüìñ Raw sample:\n\"Do not expose any part of your body to the air.\". \"I repeat..this is not a drill..\"...\n\n======================================================================\nüîß FORMATTING DATASET WITH INSTRUCTION TEMPLATE\n======================================================================\n‚úÖ Formatted 20000 stories\n\nüìñ Formatted sample:\n<s>[INST] Write a creative and chilling two-sentence horror story. [/INST] \"Do not expose any part of your body to the air.\". \"I repeat..this is not a drill..\"</s>...\n\n======================================================================\nü§ñ LOADING MISTRAL 7B INSTRUCT\n======================================================================\n\nüÜï Loading fresh mistralai/Mistral-7B-Instruct-v0.3...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e40243f5dd6c4b5a91dfa5667fc6e0da"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Base model loaded\nüìä Model device map: {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 0, 'model.layers.17': 0, 'model.layers.18': 0, 'model.layers.19': 0, 'model.layers.20': 0, 'model.layers.21': 0, 'model.layers.22': 0, 'model.layers.23': 0, 'model.layers.24': 0, 'model.layers.25': 0, 'model.layers.26': 0, 'model.layers.27': 0, 'model.layers.28': 0, 'model.layers.29': 0, 'model.layers.30': 1, 'model.layers.31': 1, 'model.norm': 1, 'model.rotary_emb': 1, 'lm_head': 1}\n‚úÖ Model prepared for training (skipping k-bit prep to save memory)\n\nüìù Loading tokenizer...\n‚úÖ Model size: ~3.76B parameters\n\n======================================================================\n‚öôÔ∏è  CONFIGURING LORA\n======================================================================\ntrainable params: 6,815,744 || all params: 7,254,839,296 || trainable%: 0.0939\n‚úÖ LoRA config applied\n   - Rank: 8\n   - Alpha: 16\n   - Target modules: 4 modules\n\n======================================================================\nüéØ TRAINING CONFIGURATION\n======================================================================\n‚úÖ Training arguments set\n   - Dataset: Part 1 (20000 stories)\n   - Epochs: 2\n   - Effective batch size: 4\n   - Learning rate: 0.0001\n   - Save every: 400 steps\n   - Max sequence length: 384\n   - Gradient checkpointing: ENABLED (saves memory)\n   - Output: /kaggle/working/llama-horror-part1\n\n======================================================================\nüöÄ INITIALIZING TRAINER\n======================================================================\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:397: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:282: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Applying formatting function to train dataset:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92ec08e5d13d42ec9b50fb2af5d42785"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Adding EOS to train dataset:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"416681c589914b0c8c499c2deac72de1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8770303c1d5746fbbc38065c0b145f02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/20000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73d3d3c8e7c34a2283b48f2132f1a4a5"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Trainer initialized\n\n======================================================================\nüî• STARTING FRESH TRAINING\nüìö Training on 20000 stories from Part 1\n======================================================================\n‚è∞ Estimated time: ~45-60 minutes...\nüí° Checkpoints will be saved every 300 steps\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='15' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   15/10000 02:40 < 34:20:42, 0.08 it/s, Epoch 0.00/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null}]}